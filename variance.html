<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Bayesian estimate of Variance</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="GZ">

    <!-- Le styles -->
    <link rel="stylesheet" href="/theme/css/bootstrap.min.css" type="text/css" />
    <style type="text/css">
      body {
        padding-top: 60px;
        padding-bottom: 40px;
      }
      .sidebar-nav {
        padding: 9px 0;
      }
      .tag-1 {
        font-size: 13pt;
      }
      .tag-2 {
        font-size: 10pt;
      }
      .tag-2 {
        font-size: 8pt;
      }
      .tag-4 {
        font-size: 6pt;
     }
    </style>
    <link href="/theme/css/bootstrap-responsive.min.css" rel="stylesheet">
        <link href="/theme/css/font-awesome.css" rel="stylesheet">

    <link href="/theme/css/pygments.css" rel="stylesheet">

    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="//html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le fav and touch icons -->
    <link rel="shortcut icon" href="/theme/images/favicon.ico">
    <link rel="apple-touch-icon" href="/theme/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/theme/images/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/theme/images/apple-touch-icon-114x114.png">

    <link href="/" type="application/atom+xml" rel="alternate" title="Random Notes ATOM Feed" />

  </head>

  <body>

    <div class="navbar navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container-fluid">
          <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="/index.html">Random Notes </a>
          <div class="nav-collapse">
            <ul class="nav">
                          <li class="divider-vertical"></li>
                  <li class="active">
                    <a href="/category/bayesian.html">
						<i class="icon-folder-open icon-large"></i>Bayesian
					</a>
                  </li>
                  <li >
                    <a href="/category/optimizatioon.html">
						<i class="icon-folder-open icon-large"></i>Optimizatioon
					</a>
                  </li>

                          <ul class="nav pull-right">
                                <li><a href="/archives.html"><i class="icon-th-list"></i>Archives</a></li>
                          </ul>

            </ul>
            <!--<p class="navbar-text pull-right">Logged in as <a href="#">username</a></p>-->
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container-fluid">
      <div class="row">
        <div class="span9" id="content">
<section id="content">
        <article>
                <header>
                        <h1>
                                <a href=""
                                        rel="bookmark"
                                        title="Permalink to Bayesian estimate of Variance">
                                        Bayesian estimate of Variance
                                </a>
                        </h1>
                </header>
                <div class="entry-content">
                <div class="well">
<footer class="post-info">
<span class="label">Date</span>
<abbr class="published" title="2021-10-01T00:00:00-05:00">
        <i class="icon-calendar"></i>Fri 01 October 2021
</abbr>
<span class="label">By</span>
<a href="/author/gz.html"><i class="icon-user"></i>GZ</a>
<span class="label">Category</span>
<a href="/category/bayesian.html"><i class="icon-folder-open"></i>Bayesian</a>.


<span class="label">Tags</span>
	<a href="/tag/bayesian.html"><i class="icon-tag"></i>bayesian</a>
</footer><!-- /.post-info -->                </div>
                <p>After observing <span class="math">\(n\)</span> data points, <span class="math">\(x_1,\dots, x_n\)</span>, we may observe another <span class="math">\(m\)</span> new data points, <span class="math">\(y_1,\dots, y_m\)</span> in the future. How can we predict the variance of these new points <span class="math">\(y_1,\dots, y_m\)</span>?</p>
<p>One way to do this is to create an estimator. We can compute the average variance of all <span class="math">\(p(x|\theta)\)</span> for different <span class="math">\(\theta\)</span>'s drawn from the posterior distribution <span class="math">\(p(\theta|x_1,\dots, x_n)\)</span>. In equation, this is <span class="math">\(\mathbb E^{\theta|x_1,\dots,x_n}[\sigma^2(\theta)]\)</span>, where <span class="math">\(\sigma^2(\theta)\)</span> is the variance of the distribution <span class="math">\(p(x|\theta)\)</span>. This is in fact a Bayes estimator because it can be obtained by minimizing the posterior expected loss on a square loss function <span class="math">\(\delta^*(x_1,\dots, x_n)=\arg \min_{\delta} \mathbb E^{\theta|x_1,\dots,x_n}[(\sigma^2(\theta)-\delta(x_1,\dots, x_n))^2]=\mathbb E^{\theta|x_1,\dots,x_n}[\sigma^2(\theta)]\)</span> . Just like the example in the slides!</p>
<p>It also makes intuitive sense. We know that “nature” uses some <span class="math">\(\theta^*\)</span> to generate the data (assuming that the model is not misspecified). We just do not know which <span class="math">\(\theta\)</span> it uses. Hence, we express our uncertainty in <span class="math">\(\theta\)</span> through the posterior distribution <span class="math">\(p(\theta|x_1,\dots, x_n)\)</span>. Since for any given <span class="math">\(\theta\)</span>, the variance of the samples is <span class="math">\(\sigma^2(\theta)\)</span>, it seems reasonable to produce an overall estimate by taking a weighted average of <span class="math">\(\sigma^2(\theta)\)</span> based on the posterior distribution of <span class="math">\(\theta\)</span>.</p>
<p>From a sampling perspective, we can imagine the computation as follows:</p>
<p>Draw a <span class="math">\(\theta’\)</span> from the posterior distribution <span class="math">\(p(\theta|x_1,\dots, x_n)\)</span>. This accounts for our uncertainty about the parameters
Draw <span class="math">\(y_1,\dots, y_m\)</span> from the distribution <span class="math">\(p(x|\theta’)\)</span> and compete the variance <span class="math">\(var(y|\theta’)\)</span>.</p>
<p>The variance <span class="math">\(var(y|\theta’)\)</span>. is the variance of future observations. It is what we want. But since we don’t know which <span class="math">\(\theta\)</span> is the true parameter, we repeat step 1 and 2 many times and take the average of the different <span class="math">\(var(y|\theta’)\)</span>s as our final estimate.</p>
<p>By contrast, the predictive variance, or the variance of the posterior predictive distribution, is not a good estimator of the variance of future samples. It means something entirely different. One way to think of the posterior predictive distribution is that we believe “nature” generates data using some <span class="math">\(\theta^*\)</span>, through <span class="math">\(p(x|\theta^)\)</span>, but we are uncertain which <span class="math">\(\theta\)</span> is <span class="math">\(\theta^*\)</span>, and so we represent our uncertainty about parameters using the posterior distribution <span class="math">\(p(\theta|x_1,\dots, x_n)\)</span>. Sampling from it involves two steps:</p>
<p>Draw a <span class="math">\(\theta’\)</span> from the posterior distribution <span class="math">\(p(\theta|x_1,\dots, x_n)\)</span>. This accounts for our uncertainty about the parameters
Draw <span class="math">\(x\)</span> from the distribution <span class="math">\(p(x|\theta’)\)</span>. This accounts for the uncertainty inherently in “nature”'s data generation process</p>
<p>Therefore, the posterior predictive distribution can be thought of what the next data point is likely to be given our observed data <span class="math">\(x_1,\dots, x_n\)</span>. The variance <span class="math">\(var(x)\)</span> for <span class="math">\(x\)</span> generated this way is the predictive variance. It incorporates the two uncertainties. As <span class="math">\(n\)</span> grows larger, we tend to be more confident about the values of <span class="math">\(\theta\)</span>, as reflected by a peakier posterior. However, note that even if the posterior converges to a Dirac distribution as <span class="math">\(n \rightarrow \infty\)</span> (Bernstein-von Mises theorem), the predictive variance will still not be 0, because it also includes the uncertainty from step 2.</p>
<p>As an extreme example, consider a Gaussian model with known variance <span class="math">\(\sigma_0^2\)</span> and a Gaussian prior. Although we do not know the mean of the Gaussian model “nature” used to generate the data, we do know the variance, and so we expect the variance of samples we observe in the future will also have variance <span class="math">\(\sigma_0^2\)</span>. But due to the uncertainty of the mean, the predictive variance will be larger than <span class="math">\(\sigma_0^2\)</span>.</p>
<p>The law of total variance can further clarify things. The predictive variance can be decomposed as:</p>
<div class="math">$$var(x) = \mathbb E[var(x|\theta)] + var(\mathbb E[x|\theta))$$</div>
<p>, where on the left hand side, the variance is taken over the posterior predictive distribution, whereas, on the right-hand side, the outer expectation and variance are taken over the posterior <span class="math">\(p(\theta|x_1,\dots, x_n)\)</span> and the inner expectation and variance are taken over <span class="math">\(p(x|\theta)\)</span>.</p>
<p>Here, the first term <span class="math">\(\mathbb E[var(x|\theta)]=\mathbb E^{\theta|x_1,\dots,x_n}[\sigma^2(\theta)]\)</span> is exactly our predicted variance of future samples, but then there is this extra term <span class="math">\(var(\mathbb E[x|\theta))\)</span> reflecting variance from parameter uncertainty.</p>
<p>This is different from the computation of the prediction for <span class="math">\(x_{n+1}\)</span>, which is the mean of the posterior predictive distribution <span class="math">\(p(x|x_1,\dots, x_n)\)</span>. As the book points out, we can alternatively first compute <span class="math">\(\mu(\theta)=\mathbb E^{X|\theta} X\)</span> and then compute <span class="math">\(\mathbb E^{\theta|x_1,\dots, x_n} \mu(\theta)\)</span> and arrive at the same result. In other words, the mean of the posterior predictive distribution and the predicted mean of future examples are the same. In fact, it is a consequence of the law of total expectation; we have <span class="math">\(\mathbb E^X[X]=\mathbb E^{\theta} \mathbb E^{X|\theta} [X]\)</span>. It is just that the law of total variance is not the same as the law of total expectation, and so, <span class="math">\(var^X(x) \ge \mathbb E^\theta[var^{X|\theta}(x|\theta)]\)</span>, the two ways of calculation are different and mean different things.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                </div><!-- /.entry-content -->
        </article>
</section>
        </div><!--/span-->

                <div class="span3 well sidebar-nav" id="sidebar">
<ul class="nav nav-list">

<li class="nav-header"><h4><i class="icon-folder-close icon-large"></i>Categories</h4></li>
<li>
<a href="/category/bayesian.html">
    <i class="icon-folder-open icon-large"></i>Bayesian
</a>
</li>
<li>
<a href="/category/optimizatioon.html">
    <i class="icon-folder-open icon-large"></i>Optimizatioon
</a>
</li>

<li class="nav-header"><h4><i class="icon-tags icon-large"></i>Tags</h4></li>
<li class="tag-1">
    <a href="/tag/optimization.html">
        <i class="icon-tag icon-large"></i>optimization
    </a>
</li>
<li class="tag-4">
    <a href="/tag/bayesian.html">
        <i class="icon-tag icon-large"></i>bayesian
    </a>
</li>


</ul>        </div><!--/.well -->

      </div><!--/row-->

      <hr>

      <footer>
        <address id="about">
                <!-- Proudly powered by <a href="http://pelican.notmyidea.org/">Pelican <i class="icon-external-link"></i></a>,
                                which takes great advantage of <a href="http://python.org">Python <i class="icon-external-link"></i></a>. -->
        </address><!-- /#about -->

        <!-- <p>The theme is from <a href="http://twitter.github.com/bootstrap/">Bootstrap from Twitter <i class="icon-external-link"></i></a>,
                   and <a href="http://fortawesome.github.com/Font-Awesome/">Font-Awesome <i class="icon-external-link"></i></a>, thanks!</p> -->
      </footer>

    </div><!--/.fluid-container-->



    <!-- Le javascript -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="/theme/js/jquery-1.7.2.min.js"></script>
    <script src="/theme/js/bootstrap.min.js"></script>
  </body>
</html>